{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importando Bibliotecas necessárias"
      ],
      "metadata": {
        "id": "eksOypP4k1dT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U \"tensorflow==2.8.*\"\n",
        "!pip install transformers==4.37.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mx9-iXhmuLib",
        "outputId": "dc498496-de9c-4eb3-db80-41ec1e4cdf61"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.37.2 in /usr/local/lib/python3.10/dist-packages (4.37.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Zc3cH3J5kRE9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pré processamento de dados"
      ],
      "metadata": {
        "id": "AlcoWYVek5hi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregando dataset\n",
        "dados = pd.read_csv('amazon_alexa.tsv', sep='\\t')\n",
        "\n",
        "# Eliminando valores em branco nas colunas que serão utilizadas\n",
        "dados.dropna(subset=['verified_reviews', 'feedback'], inplace=True)\n",
        "\n",
        "#\n",
        "dados['feedback'] = dados['feedback'].apply(lambda x: 'positive' if x == 1 else 'negative')\n",
        "X = dados['verified_reviews']\n",
        "y = dados['feedback']\n",
        "\n",
        "# Rotulação\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Criando dataset de treino e validação\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "0fLZ-WfIk0fT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelo 01 - SVM com Bag of Words(BOW)"
      ],
      "metadata": {
        "id": "yCQhVXAamqcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando Pipeline SVM com BOW\n",
        "pipeline_bow = make_pipeline(CountVectorizer(), SVC(kernel='linear'))\n",
        "\n",
        "# Treinando modelo 02\n",
        "pipeline_bow.fit(X_train, y_train)\n",
        "\n",
        "# Extraindo métricas do modelo 01\n",
        "y_pred_bow = pipeline_bow.predict(X_test)\n",
        "print(\"Métricas do modelo 01:\\n\", classification_report(y_test, y_pred_bow))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PybG0QfBm0jD",
        "outputId": "16608f00-7981-470b-bf3a-7059958d95f0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Métricas do modelo 01:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.43      0.54        92\n",
            "           1       0.94      0.98      0.96       853\n",
            "\n",
            "    accuracy                           0.93       945\n",
            "   macro avg       0.82      0.71      0.75       945\n",
            "weighted avg       0.92      0.93      0.92       945\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelo 02 - SVM com Embeddings"
      ],
      "metadata": {
        "id": "ze45IoG6m48U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separando os dados de treino em vetor\n",
        "sentences = [review.split() for review in X_train]\n",
        "\n",
        "# Criando o modelo a partir do vetor\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Função para obter os embeddings do dataset\n",
        "def get_avg_word2vec(review, model, num_features):\n",
        "    words = review.split()\n",
        "    feature_vec = np.zeros((num_features,), dtype=\"float32\")\n",
        "    n_words = 0\n",
        "    for word in words:\n",
        "        if word in model.wv:\n",
        "            n_words += 1\n",
        "            feature_vec = np.add(feature_vec, model.wv[word])\n",
        "    if n_words > 0:\n",
        "        feature_vec = np.divide(feature_vec, n_words)\n",
        "    return feature_vec\n",
        "\n",
        "# Transformando o dataset em vetores word embeddings\n",
        "X_train_word2vec = np.array([get_avg_word2vec(review, word2vec_model, 100) for review in X_train])\n",
        "X_test_word2vec = np.array([get_avg_word2vec(review, word2vec_model, 100) for review in X_test])\n",
        "\n",
        "# Treinando modelo 02\n",
        "svm_word2vec = SVC(kernel='linear')\n",
        "svm_word2vec.fit(X_train_word2vec, y_train)\n",
        "\n",
        "# Extraindo métricas do modelo 02\n",
        "y_pred_word2vec = svm_word2vec.predict(X_test_word2vec)\n",
        "print(\"Métricas do modelo 02:\\n\", classification_report(y_test, y_pred_word2vec))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zckL-vJxm_te",
        "outputId": "00245c9d-3414-44a3-be9d-04dcf8d69b56"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Métricas do modelo 02:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        92\n",
            "           1       0.90      1.00      0.95       853\n",
            "\n",
            "    accuracy                           0.90       945\n",
            "   macro avg       0.45      0.50      0.47       945\n",
            "weighted avg       0.81      0.90      0.86       945\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelo 03 - BERT"
      ],
      "metadata": {
        "id": "S6D_O5finCo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregando BERT\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Carregando Modelo\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Tokenrizando dados\n",
        "train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer(list(X_test), truncation=True, padding=True, max_length=128)\n",
        "\n",
        "# Convertendo o dataset para tensor\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).shuffle(1000).batch(16)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(16)\n",
        "\n",
        "# Compilando modelo\n",
        "optimizer = Adam(learning_rate=3e-5)\n",
        "loss = SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = SparseCategoricalAccuracy('accuracy')\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        "\n",
        "# Treinando o modelo\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "model.fit(train_dataset, validation_data=test_dataset, epochs=10, callbacks=[early_stopping])\n",
        "\n",
        "# Extraindo métricas do modelo 03\n",
        "print(\"Métricas do modelo 03:\\n\", model.evaluate(test_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tK17Ri5ZnI7x",
        "outputId": "a6e461c2-dfdc-4906-989a-3c51dbd3c92e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "138/138 [==============================] - 881s 6s/step - loss: 0.2408 - accuracy: 0.9147 - val_loss: 0.1387 - val_accuracy: 0.9323\n",
            "Epoch 2/10\n",
            "138/138 [==============================] - 852s 6s/step - loss: 0.1009 - accuracy: 0.9601 - val_loss: 0.1345 - val_accuracy: 0.9397\n",
            "Epoch 3/10\n",
            "138/138 [==============================] - 860s 6s/step - loss: 0.0544 - accuracy: 0.9791 - val_loss: 0.1887 - val_accuracy: 0.9376\n",
            "Epoch 4/10\n",
            "138/138 [==============================] - 841s 6s/step - loss: 0.0374 - accuracy: 0.9859 - val_loss: 0.1837 - val_accuracy: 0.9503\n",
            "Epoch 5/10\n",
            "138/138 [==============================] - 852s 6s/step - loss: 0.0157 - accuracy: 0.9932 - val_loss: 0.2490 - val_accuracy: 0.9513\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.2490 - accuracy: 0.9513\n",
            "Métricas do modelo 03:\n",
            " [0.24900774657726288, 0.9513227343559265]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DHdk6mRsuIVP"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}